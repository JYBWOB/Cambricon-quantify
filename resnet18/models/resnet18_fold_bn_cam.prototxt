layer {
name:"input_1"
top:"input_1"
type: "Input"
input_param {
    shape {
        dim: 1
        dim: 3
        dim: 224
        dim: 224
    }
}
}
layer {
name:"Q_bn_data"
top:"Q_bn_data"
type: "BatchNorm"
bottom: "input_1"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_bn_data_scale"
type: "Scale"
bottom: "Q_bn_data"
top: "Q_bn_data"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_conv0"
top:"Q_conv0"
type: "Convolution"
bottom: "Q_bn_data"
convolution_param {
    num_output: 64
    pad_h: 3
    pad_w: 3
    kernel_size: 7
    stride: 2
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 27.039844
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 20.215049
}
}
layer {
name:"Q_relu0"
top:"Q_relu0"
type: "ReLU"
bottom: "Q_conv0"
}
layer {
name:"Q_pooling0"
top:"Q_pooling0"
type: "Pooling"
bottom: "Q_relu0"
pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad_h: 1
    pad_w: 1
}
}
layer {
  name:"slice1"
  type:"Slice"
  bottom:"Q_pooling0"
  top:"slice1"
  top:"slice_tmp1"
  slice_param {
    axis:2
    slice_point:56
  }
}

layer {
  name:"slice2"
  type:"Slice"
  bottom:"slice1"
  top:"slice2"
  top:"slice_tmp2"
  slice_param {
    axis:3
    slice_point:56
  }
}

layer {
name:"Q_stage1_unit1_bn1"
top:"Q_stage1_unit1_bn1"
type: "BatchNorm"
bottom: "slice2"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_stage1_unit1_bn1_scale"
type: "Scale"
bottom: "Q_stage1_unit1_bn1"
top: "Q_stage1_unit1_bn1"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_stage1_unit1_relu1"
top:"Q_stage1_unit1_relu1"
type: "ReLU"
bottom: "Q_stage1_unit1_bn1"
}
layer {
name:"Q_stage1_unit1_conv1"
top:"Q_stage1_unit1_conv1"
type: "Convolution"
bottom: "Q_stage1_unit1_relu1"
convolution_param {
    num_output: 64
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 50.398650
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 62.474265
}
}
layer {
name:"Q_stage1_unit1_relu2"
top:"Q_stage1_unit1_relu2"
type: "ReLU"
bottom: "Q_stage1_unit1_conv1"
}
layer {
name:"Q_stage1_unit1_conv2"
top:"Q_stage1_unit1_conv2"
type: "Convolution"
bottom: "Q_stage1_unit1_relu2"
convolution_param {
    num_output: 64
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 85.974543
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 68.415728
}
}
layer {
name:"Q_stage1_unit1_sc"
top:"Q_stage1_unit1_sc"
type: "Convolution"
bottom: "Q_stage1_unit1_relu1"
convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 49.333122
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 26.559153
}
}
layer {
name:"Q_add_1"
top:"Q_add_1"
bottom: "Q_stage1_unit1_conv2"
bottom: "Q_stage1_unit1_sc"
type: "Eltwise"
eltwise_param {
    operation: SUM
}
}
layer {
name:"Q_stage1_unit2_bn1"
top:"Q_stage1_unit2_bn1"
type: "BatchNorm"
bottom: "Q_add_1"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_stage1_unit2_bn1_scale"
type: "Scale"
bottom: "Q_stage1_unit2_bn1"
top: "Q_stage1_unit2_bn1"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_stage1_unit2_relu1"
top:"Q_stage1_unit2_relu1"
type: "ReLU"
bottom: "Q_stage1_unit2_bn1"
}
layer {
name:"Q_stage1_unit2_conv1"
top:"Q_stage1_unit2_conv1"
type: "Convolution"
bottom: "Q_stage1_unit2_relu1"
convolution_param {
    num_output: 64
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 35.680729
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 52.292737
}
}
layer {
name:"Q_stage1_unit2_relu2"
top:"Q_stage1_unit2_relu2"
type: "ReLU"
bottom: "Q_stage1_unit2_conv1"
}
layer {
name:"Q_stage1_unit2_conv2"
top:"Q_stage1_unit2_conv2"
type: "Convolution"
bottom: "Q_stage1_unit2_relu2"
convolution_param {
    num_output: 64
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 38.527330
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 53.854579
}
}
layer {
name:"Q_add_2"
top:"Q_add_2"
bottom: "Q_stage1_unit2_conv2"
bottom: "Q_add_1"
type: "Eltwise"
eltwise_param {
    operation: SUM
}
}
layer {
name:"Q_stage2_unit1_bn1"
top:"Q_stage2_unit1_bn1"
type: "BatchNorm"
bottom: "Q_add_2"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_stage2_unit1_bn1_scale"
type: "Scale"
bottom: "Q_stage2_unit1_bn1"
top: "Q_stage2_unit1_bn1"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_stage2_unit1_relu1"
top:"Q_stage2_unit1_relu1"
type: "ReLU"
bottom: "Q_stage2_unit1_bn1"
}
layer {
name:"Q_stage2_unit1_conv1"
top:"Q_stage2_unit1_conv1"
type: "Convolution"
bottom: "Q_stage2_unit1_relu1"
convolution_param {
    num_output: 128
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 2
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 58.163733
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 64.594250
}
}
layer {
name:"Q_stage2_unit1_relu2"
top:"Q_stage2_unit1_relu2"
type: "ReLU"
bottom: "Q_stage2_unit1_conv1"
}
layer {
name:"Q_stage2_unit1_conv2"
top:"Q_stage2_unit1_conv2"
type: "Convolution"
bottom: "Q_stage2_unit1_relu2"
convolution_param {
    num_output: 128
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 165.353631
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 78.785801
}
}
layer {
name:"Q_stage2_unit1_sc"
top:"Q_stage2_unit1_sc"
type: "Convolution"
bottom: "Q_stage2_unit1_relu1"
convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 2
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 57.382507
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 45.293690
}
}
layer {
name:"Q_add_3"
top:"Q_add_3"
bottom: "Q_stage2_unit1_conv2"
bottom: "Q_stage2_unit1_sc"
type: "Eltwise"
eltwise_param {
    operation: SUM
}
}
layer {
name:"Q_stage2_unit2_bn1"
top:"Q_stage2_unit2_bn1"
type: "BatchNorm"
bottom: "Q_add_3"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_stage2_unit2_bn1_scale"
type: "Scale"
bottom: "Q_stage2_unit2_bn1"
top: "Q_stage2_unit2_bn1"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_stage2_unit2_relu1"
top:"Q_stage2_unit2_relu1"
type: "ReLU"
bottom: "Q_stage2_unit2_bn1"
}
layer {
name:"Q_stage2_unit2_conv1"
top:"Q_stage2_unit2_conv1"
type: "Convolution"
bottom: "Q_stage2_unit2_relu1"
convolution_param {
    num_output: 128
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 75.061929
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 83.474615
}
}
layer {
name:"Q_stage2_unit2_relu2"
top:"Q_stage2_unit2_relu2"
type: "ReLU"
bottom: "Q_stage2_unit2_conv1"
}
layer {
name:"Q_stage2_unit2_conv2"
top:"Q_stage2_unit2_conv2"
type: "Convolution"
bottom: "Q_stage2_unit2_relu2"
convolution_param {
    num_output: 128
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 93.156837
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 89.492580
}
}
layer {
name:"Q_add_4"
top:"Q_add_4"
bottom: "Q_stage2_unit2_conv2"
bottom: "Q_add_3"
type: "Eltwise"
eltwise_param {
    operation: SUM
}
}
layer {
name:"Q_stage3_unit1_bn1"
top:"Q_stage3_unit1_bn1"
type: "BatchNorm"
bottom: "Q_add_4"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_stage3_unit1_bn1_scale"
type: "Scale"
bottom: "Q_stage3_unit1_bn1"
top: "Q_stage3_unit1_bn1"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_stage3_unit1_relu1"
top:"Q_stage3_unit1_relu1"
type: "ReLU"
bottom: "Q_stage3_unit1_bn1"
}
layer {
name:"Q_stage3_unit1_conv1"
top:"Q_stage3_unit1_conv1"
type: "Convolution"
bottom: "Q_stage3_unit1_relu1"
convolution_param {
    num_output: 256
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 2
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 114.501695
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 96.194703
}
}
layer {
name:"Q_stage3_unit1_relu2"
top:"Q_stage3_unit1_relu2"
type: "ReLU"
bottom: "Q_stage3_unit1_conv1"
}
layer {
name:"Q_stage3_unit1_conv2"
top:"Q_stage3_unit1_conv2"
type: "Convolution"
bottom: "Q_stage3_unit1_relu2"
convolution_param {
    num_output: 256
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 134.101370
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 114.786817
}
}
layer {
name:"Q_stage3_unit1_sc"
top:"Q_stage3_unit1_sc"
type: "Convolution"
bottom: "Q_stage3_unit1_relu1"
convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 2
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 109.442805
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 79.971494
}
}
layer {
name:"Q_add_5"
top:"Q_add_5"
bottom: "Q_stage3_unit1_conv2"
bottom: "Q_stage3_unit1_sc"
type: "Eltwise"
eltwise_param {
    operation: SUM
}
}
layer {
name:"Q_stage3_unit2_bn1"
top:"Q_stage3_unit2_bn1"
type: "BatchNorm"
bottom: "Q_add_5"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_stage3_unit2_bn1_scale"
type: "Scale"
bottom: "Q_stage3_unit2_bn1"
top: "Q_stage3_unit2_bn1"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_stage3_unit2_relu1"
top:"Q_stage3_unit2_relu1"
type: "ReLU"
bottom: "Q_stage3_unit2_bn1"
}
layer {
name:"Q_stage3_unit2_conv1"
top:"Q_stage3_unit2_conv1"
type: "Convolution"
bottom: "Q_stage3_unit2_relu1"
convolution_param {
    num_output: 256
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 91.863806
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 127.497812
}
}
layer {
name:"Q_stage3_unit2_relu2"
top:"Q_stage3_unit2_relu2"
type: "ReLU"
bottom: "Q_stage3_unit2_conv1"
}
layer {
name:"Q_stage3_unit2_conv2"
top:"Q_stage3_unit2_conv2"
type: "Convolution"
bottom: "Q_stage3_unit2_relu2"
convolution_param {
    num_output: 256
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 98.596917
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 131.415643
}
}
layer {
name:"Q_add_6"
top:"Q_add_6"
bottom: "Q_stage3_unit2_conv2"
bottom: "Q_add_5"
type: "Eltwise"
eltwise_param {
    operation: SUM
}
}
layer {
name:"Q_stage4_unit1_bn1"
top:"Q_stage4_unit1_bn1"
type: "BatchNorm"
bottom: "Q_add_6"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_stage4_unit1_bn1_scale"
type: "Scale"
bottom: "Q_stage4_unit1_bn1"
top: "Q_stage4_unit1_bn1"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_stage4_unit1_relu1"
top:"Q_stage4_unit1_relu1"
type: "ReLU"
bottom: "Q_stage4_unit1_bn1"
}
layer {
name:"Q_stage4_unit1_conv1"
top:"Q_stage4_unit1_conv1"
type: "Convolution"
bottom: "Q_stage4_unit1_relu1"
convolution_param {
    num_output: 512
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 2
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 154.210647
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 145.255483
}
}
layer {
name:"Q_stage4_unit1_relu2"
top:"Q_stage4_unit1_relu2"
type: "ReLU"
bottom: "Q_stage4_unit1_conv1"
}
layer {
name:"Q_stage4_unit1_conv2"
top:"Q_stage4_unit1_conv2"
type: "Convolution"
bottom: "Q_stage4_unit1_relu2"
convolution_param {
    num_output: 512
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 182.099662
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 178.128406
}
}
layer {
name:"Q_stage4_unit1_sc"
top:"Q_stage4_unit1_sc"
type: "Convolution"
bottom: "Q_stage4_unit1_relu1"
convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 2
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 139.026480
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 101.767563
}
}
layer {
name:"Q_add_7"
top:"Q_add_7"
bottom: "Q_stage4_unit1_conv2"
bottom: "Q_stage4_unit1_sc"
type: "Eltwise"
eltwise_param {
    operation: SUM
}
}
layer {
name:"Q_stage4_unit2_bn1"
top:"Q_stage4_unit2_bn1"
type: "BatchNorm"
bottom: "Q_add_7"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_stage4_unit2_bn1_scale"
type: "Scale"
bottom: "Q_stage4_unit2_bn1"
top: "Q_stage4_unit2_bn1"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_stage4_unit2_relu1"
top:"Q_stage4_unit2_relu1"
type: "ReLU"
bottom: "Q_stage4_unit2_bn1"
}
layer {
name:"Q_stage4_unit2_conv1"
top:"Q_stage4_unit2_conv1"
type: "Convolution"
bottom: "Q_stage4_unit2_relu1"
convolution_param {
    num_output: 512
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 182.566648
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 179.119417
}
}
layer {
name:"Q_stage4_unit2_relu2"
top:"Q_stage4_unit2_relu2"
type: "ReLU"
bottom: "Q_stage4_unit2_conv1"
}
layer {
name:"Q_stage4_unit2_conv2"
top:"Q_stage4_unit2_conv2"
type: "Convolution"
bottom: "Q_stage4_unit2_relu2"
convolution_param {
    num_output: 512
    pad_h: 1
    pad_w: 1
    kernel_size: 3
    stride: 1
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 128.024709
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 201.807841
}
}
layer {
name:"Q_add_8"
top:"Q_add_8"
bottom: "Q_stage4_unit2_conv2"
bottom: "Q_add_7"
type: "Eltwise"
eltwise_param {
    operation: SUM
}
}
layer {
name:"Q_bn1"
top:"Q_bn1"
type: "BatchNorm"
bottom: "Q_add_8"
batch_norm_param {
    moving_average_fraction: 0.990000
    eps: 0.001
}
}
layer {
name: "Q_bn1_scale"
type: "Scale"
bottom: "Q_bn1"
top: "Q_bn1"
scale_param {
    bias_term: true
}
}
layer {
name:"Q_relu1"
top:"Q_relu1"
type: "ReLU"
bottom: "Q_bn1"
}
layer {
name:"Q_pool1"
top:"Q_pool1"
type: "Pooling"
bottom: "Q_relu1"
pooling_param {
    pool: AVE
    global_pooling: true
}
}
layer {
name:"Q_fc1"
top:"Q_fc1"
type: "InnerProduct"
bottom: "Q_pool1"
inner_product_param {
    num_output: 1000
}
bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 146.300763
}
blobs_dtype {
    type: DT_INT8
    position: 0
    scale: 74.840031
}
}
layer {
name:"Q_softmax"
top:"Q_softmax"
type: "Softmax"
bottom: "Q_fc1"
}
