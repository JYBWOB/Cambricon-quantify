layer {
  name: "data"
  type: "Input"
  top: "input_1"
  input_param {
    shape {
      dim: 50
      dim: 3
      dim: 224
      dim: 224
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "input_1"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 7
    stride: 2
    pad_h: 3
    pad_w: 3
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.69
    std: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: 1
    scale: 1.6814399
  }
  blobs_dtype {
    type: DT_INT8
    position: -7
    scale: 1.4087094
  }
}
layer {
  name: "bn_conv1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn_conv1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn_conv1_scale"
  type: "Scale"
  bottom: "bn_conv1"
  top: "bn_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_1"
  type: "ReLU"
  bottom: "bn_conv1"
  top: "activation_1"
}
layer {
  name: "max_pooling2d_1"
  type: "Pooling"
  bottom: "activation_1"
  top: "max_pooling2d_1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad_h: 1
    pad_w: 1
  }
}
layer {
  name: "slice1"
  type: "Slice"
  bottom: "max_pooling2d_1"
  top: "slice1"
  top: "slice_tmp1"
  slice_param {
    slice_point: 56
    axis: 2
  }
}
layer {
  name: "slice2"
  type: "Slice"
  bottom: "slice1"
  top: "slice2"
  top: "slice_tmp2"
  slice_param {
    slice_point: 56
    axis: 3
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "slice2"
  top: "res2a_branch2a"
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.2702876
  }
  blobs_dtype {
    type: DT_INT8
    position: -7
    scale: 1.3828479
  }
}
layer {
  name: "bn2a_branch2a"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "bn2a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2a_branch2a_scale"
  type: "Scale"
  bottom: "bn2a_branch2a"
  top: "bn2a_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_2"
  type: "ReLU"
  bottom: "bn2a_branch2a"
  top: "activation_2"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "activation_2"
  top: "res2a_branch2b"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.6738182
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.271904
  }
}
layer {
  name: "bn2a_branch2b"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "bn2a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2a_branch2b_scale"
  type: "Scale"
  bottom: "bn2a_branch2b"
  top: "bn2a_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_3"
  type: "ReLU"
  bottom: "bn2a_branch2b"
  top: "activation_3"
}
layer {
  name: "res2a_branch2c"
  type: "Convolution"
  bottom: "activation_3"
  top: "res2a_branch2c"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.2365816
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.2482163
  }
}
layer {
  name: "res2a_branch1"
  type: "Convolution"
  bottom: "slice2"
  top: "res2a_branch1"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.2702876
  }
  blobs_dtype {
    type: DT_INT8
    position: -7
    scale: 1.1019814
  }
}
layer {
  name: "bn2a_branch2c"
  type: "BatchNorm"
  bottom: "res2a_branch2c"
  top: "bn2a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2a_branch2c_scale"
  type: "Scale"
  bottom: "bn2a_branch2c"
  top: "bn2a_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "bn2a_branch1"
  type: "BatchNorm"
  bottom: "res2a_branch1"
  top: "bn2a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2a_branch1_scale"
  type: "Scale"
  bottom: "bn2a_branch1"
  top: "bn2a_branch1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_1"
  type: "Eltwise"
  bottom: "bn2a_branch2c"
  bottom: "bn2a_branch1"
  top: "add_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_4"
  type: "ReLU"
  bottom: "add_1"
  top: "activation_4"
}
layer {
  name: "res2b_branch2a"
  type: "Convolution"
  bottom: "activation_4"
  top: "res2b_branch2a"
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.2712417
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.6711503
  }
}
layer {
  name: "bn2b_branch2a"
  type: "BatchNorm"
  bottom: "res2b_branch2a"
  top: "bn2b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2b_branch2a_scale"
  type: "Scale"
  bottom: "bn2b_branch2a"
  top: "bn2b_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_5"
  type: "ReLU"
  bottom: "bn2b_branch2a"
  top: "activation_5"
}
layer {
  name: "res2b_branch2b"
  type: "Convolution"
  bottom: "activation_5"
  top: "res2b_branch2b"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.0280696
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.5586338
  }
}
layer {
  name: "bn2b_branch2b"
  type: "BatchNorm"
  bottom: "res2b_branch2b"
  top: "bn2b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2b_branch2b_scale"
  type: "Scale"
  bottom: "bn2b_branch2b"
  top: "bn2b_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_6"
  type: "ReLU"
  bottom: "bn2b_branch2b"
  top: "activation_6"
}
layer {
  name: "res2b_branch2c"
  type: "Convolution"
  bottom: "activation_6"
  top: "res2b_branch2c"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.1167599
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.7744814
  }
}
layer {
  name: "bn2b_branch2c"
  type: "BatchNorm"
  bottom: "res2b_branch2c"
  top: "bn2b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2b_branch2c_scale"
  type: "Scale"
  bottom: "bn2b_branch2c"
  top: "bn2b_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_2"
  type: "Eltwise"
  bottom: "bn2b_branch2c"
  bottom: "activation_4"
  top: "add_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_7"
  type: "ReLU"
  bottom: "add_2"
  top: "activation_7"
}
layer {
  name: "res2c_branch2a"
  type: "Convolution"
  bottom: "activation_7"
  top: "res2c_branch2a"
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.2633468
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.8812097
  }
}
layer {
  name: "bn2c_branch2a"
  type: "BatchNorm"
  bottom: "res2c_branch2a"
  top: "bn2c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2c_branch2a_scale"
  type: "Scale"
  bottom: "bn2c_branch2a"
  top: "bn2c_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_8"
  type: "ReLU"
  bottom: "bn2c_branch2a"
  top: "activation_8"
}
layer {
  name: "res2c_branch2b"
  type: "Convolution"
  bottom: "activation_8"
  top: "res2c_branch2b"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.0214795
  }
  blobs_dtype {
    type: DT_INT8
    position: -9
    scale: 1.138068
  }
}
layer {
  name: "bn2c_branch2b"
  type: "BatchNorm"
  bottom: "res2c_branch2b"
  top: "bn2c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2c_branch2b_scale"
  type: "Scale"
  bottom: "bn2c_branch2b"
  top: "bn2c_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_9"
  type: "ReLU"
  bottom: "bn2c_branch2b"
  top: "activation_9"
}
layer {
  name: "res2c_branch2c"
  type: "Convolution"
  bottom: "activation_9"
  top: "res2c_branch2c"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.6865619
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.417477
  }
}
layer {
  name: "bn2c_branch2c"
  type: "BatchNorm"
  bottom: "res2c_branch2c"
  top: "bn2c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn2c_branch2c_scale"
  type: "Scale"
  bottom: "bn2c_branch2c"
  top: "bn2c_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_3"
  type: "Eltwise"
  bottom: "bn2c_branch2c"
  bottom: "activation_7"
  top: "add_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_10"
  type: "ReLU"
  bottom: "add_3"
  top: "activation_10"
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "activation_10"
  top: "res3a_branch2a"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 2
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.1931243
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.4855993
  }
}
layer {
  name: "bn3a_branch2a"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "bn3a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3a_branch2a_scale"
  type: "Scale"
  bottom: "bn3a_branch2a"
  top: "bn3a_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_11"
  type: "ReLU"
  bottom: "bn3a_branch2a"
  top: "activation_11"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "activation_11"
  top: "res3a_branch2b"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.1318332
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.2929417
  }
}
layer {
  name: "bn3a_branch2b"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "bn3a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3a_branch2b_scale"
  type: "Scale"
  bottom: "bn3a_branch2b"
  top: "bn3a_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_12"
  type: "ReLU"
  bottom: "bn3a_branch2b"
  top: "activation_12"
}
layer {
  name: "res3a_branch2c"
  type: "Convolution"
  bottom: "activation_12"
  top: "res3a_branch2c"
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.4232231
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.1421479
  }
}
layer {
  name: "res3a_branch1"
  type: "Convolution"
  bottom: "activation_10"
  top: "res3a_branch1"
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 2
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.1931243
  }
  blobs_dtype {
    type: DT_INT8
    position: -7
    scale: 1.5443299
  }
}
layer {
  name: "bn3a_branch2c"
  type: "BatchNorm"
  bottom: "res3a_branch2c"
  top: "bn3a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3a_branch2c_scale"
  type: "Scale"
  bottom: "bn3a_branch2c"
  top: "bn3a_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "bn3a_branch1"
  type: "BatchNorm"
  bottom: "res3a_branch1"
  top: "bn3a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3a_branch1_scale"
  type: "Scale"
  bottom: "bn3a_branch1"
  top: "bn3a_branch1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_4"
  type: "Eltwise"
  bottom: "bn3a_branch2c"
  bottom: "bn3a_branch1"
  top: "add_4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_13"
  type: "ReLU"
  bottom: "add_4"
  top: "activation_13"
}
layer {
  name: "res3b_branch2a"
  type: "Convolution"
  bottom: "activation_13"
  top: "res3b_branch2a"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.1678435
  }
  blobs_dtype {
    type: DT_INT8
    position: -9
    scale: 1.2739841
  }
}
layer {
  name: "bn3b_branch2a"
  type: "BatchNorm"
  bottom: "res3b_branch2a"
  top: "bn3b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3b_branch2a_scale"
  type: "Scale"
  bottom: "bn3b_branch2a"
  top: "bn3b_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_14"
  type: "ReLU"
  bottom: "bn3b_branch2a"
  top: "activation_14"
}
layer {
  name: "res3b_branch2b"
  type: "Convolution"
  bottom: "activation_14"
  top: "res3b_branch2b"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.1643395
  }
  blobs_dtype {
    type: DT_INT8
    position: -9
    scale: 1.3996733
  }
}
layer {
  name: "bn3b_branch2b"
  type: "BatchNorm"
  bottom: "res3b_branch2b"
  top: "bn3b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3b_branch2b_scale"
  type: "Scale"
  bottom: "bn3b_branch2b"
  top: "bn3b_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_15"
  type: "ReLU"
  bottom: "bn3b_branch2b"
  top: "activation_15"
}
layer {
  name: "res3b_branch2c"
  type: "Convolution"
  bottom: "activation_15"
  top: "res3b_branch2c"
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.2514589
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.4410509
  }
}
layer {
  name: "bn3b_branch2c"
  type: "BatchNorm"
  bottom: "res3b_branch2c"
  top: "bn3b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3b_branch2c_scale"
  type: "Scale"
  bottom: "bn3b_branch2c"
  top: "bn3b_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_5"
  type: "Eltwise"
  bottom: "bn3b_branch2c"
  bottom: "activation_13"
  top: "add_5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_16"
  type: "ReLU"
  bottom: "add_5"
  top: "activation_16"
}
layer {
  name: "res3c_branch2a"
  type: "Convolution"
  bottom: "activation_16"
  top: "res3c_branch2a"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.173703
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.3462325
  }
}
layer {
  name: "bn3c_branch2a"
  type: "BatchNorm"
  bottom: "res3c_branch2a"
  top: "bn3c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3c_branch2a_scale"
  type: "Scale"
  bottom: "bn3c_branch2a"
  top: "bn3c_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_17"
  type: "ReLU"
  bottom: "bn3c_branch2a"
  top: "activation_17"
}
layer {
  name: "res3c_branch2b"
  type: "Convolution"
  bottom: "activation_17"
  top: "res3c_branch2b"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.1770828
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.3277117
  }
}
layer {
  name: "bn3c_branch2b"
  type: "BatchNorm"
  bottom: "res3c_branch2b"
  top: "bn3c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3c_branch2b_scale"
  type: "Scale"
  bottom: "bn3c_branch2b"
  top: "bn3c_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_18"
  type: "ReLU"
  bottom: "bn3c_branch2b"
  top: "activation_18"
}
layer {
  name: "res3c_branch2c"
  type: "Convolution"
  bottom: "activation_18"
  top: "res3c_branch2c"
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 1.8223357
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.7252349
  }
}
layer {
  name: "bn3c_branch2c"
  type: "BatchNorm"
  bottom: "res3c_branch2c"
  top: "bn3c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3c_branch2c_scale"
  type: "Scale"
  bottom: "bn3c_branch2c"
  top: "bn3c_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_6"
  type: "Eltwise"
  bottom: "bn3c_branch2c"
  bottom: "activation_16"
  top: "add_6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_19"
  type: "ReLU"
  bottom: "add_6"
  top: "activation_19"
}
layer {
  name: "res3d_branch2a"
  type: "Convolution"
  bottom: "activation_19"
  top: "res3d_branch2a"
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.2111969
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.4326999
  }
}
layer {
  name: "bn3d_branch2a"
  type: "BatchNorm"
  bottom: "res3d_branch2a"
  top: "bn3d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3d_branch2a_scale"
  type: "Scale"
  bottom: "bn3d_branch2a"
  top: "bn3d_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_20"
  type: "ReLU"
  bottom: "bn3d_branch2a"
  top: "activation_20"
}
layer {
  name: "res3d_branch2b"
  type: "Convolution"
  bottom: "activation_20"
  top: "res3d_branch2b"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.8674189
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.8224646
  }
}
layer {
  name: "bn3d_branch2b"
  type: "BatchNorm"
  bottom: "res3d_branch2b"
  top: "bn3d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3d_branch2b_scale"
  type: "Scale"
  bottom: "bn3d_branch2b"
  top: "bn3d_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_21"
  type: "ReLU"
  bottom: "bn3d_branch2b"
  top: "activation_21"
}
layer {
  name: "res3d_branch2c"
  type: "Convolution"
  bottom: "activation_21"
  top: "res3d_branch2c"
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.5093429
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.766525
  }
}
layer {
  name: "bn3d_branch2c"
  type: "BatchNorm"
  bottom: "res3d_branch2c"
  top: "bn3d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn3d_branch2c_scale"
  type: "Scale"
  bottom: "bn3d_branch2c"
  top: "bn3d_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_7"
  type: "Eltwise"
  bottom: "bn3d_branch2c"
  bottom: "activation_19"
  top: "add_7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_22"
  type: "ReLU"
  bottom: "add_7"
  top: "activation_22"
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "activation_22"
  top: "res4a_branch2a"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 2
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.123368
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.4556786
  }
}
layer {
  name: "bn4a_branch2a"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "bn4a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4a_branch2a_scale"
  type: "Scale"
  bottom: "bn4a_branch2a"
  top: "bn4a_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_23"
  type: "ReLU"
  bottom: "bn4a_branch2a"
  top: "activation_23"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "activation_23"
  top: "res4a_branch2b"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.5783643
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.8559829
  }
}
layer {
  name: "bn4a_branch2b"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "bn4a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4a_branch2b_scale"
  type: "Scale"
  bottom: "bn4a_branch2b"
  top: "bn4a_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_24"
  type: "ReLU"
  bottom: "bn4a_branch2b"
  top: "activation_24"
}
layer {
  name: "res4a_branch2c"
  type: "Convolution"
  bottom: "activation_24"
  top: "res4a_branch2c"
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.9660971
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.2912676
  }
}
layer {
  name: "res4a_branch1"
  type: "Convolution"
  bottom: "activation_22"
  top: "res4a_branch1"
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    stride: 2
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.123368
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.1790868
  }
}
layer {
  name: "bn4a_branch2c"
  type: "BatchNorm"
  bottom: "res4a_branch2c"
  top: "bn4a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4a_branch2c_scale"
  type: "Scale"
  bottom: "bn4a_branch2c"
  top: "bn4a_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "bn4a_branch1"
  type: "BatchNorm"
  bottom: "res4a_branch1"
  top: "bn4a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4a_branch1_scale"
  type: "Scale"
  bottom: "bn4a_branch1"
  top: "bn4a_branch1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_8"
  type: "Eltwise"
  bottom: "bn4a_branch2c"
  bottom: "bn4a_branch1"
  top: "add_8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_25"
  type: "ReLU"
  bottom: "add_8"
  top: "activation_25"
}
layer {
  name: "res4b_branch2a"
  type: "Convolution"
  bottom: "activation_25"
  top: "res4b_branch2a"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.1142207
  }
  blobs_dtype {
    type: DT_INT8
    position: -9
    scale: 1.0513577
  }
}
layer {
  name: "bn4b_branch2a"
  type: "BatchNorm"
  bottom: "res4b_branch2a"
  top: "bn4b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4b_branch2a_scale"
  type: "Scale"
  bottom: "bn4b_branch2a"
  top: "bn4b_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_26"
  type: "ReLU"
  bottom: "bn4b_branch2a"
  top: "activation_26"
}
layer {
  name: "res4b_branch2b"
  type: "Convolution"
  bottom: "activation_26"
  top: "res4b_branch2b"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.5393426
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.1370775
  }
}
layer {
  name: "bn4b_branch2b"
  type: "BatchNorm"
  bottom: "res4b_branch2b"
  top: "bn4b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4b_branch2b_scale"
  type: "Scale"
  bottom: "bn4b_branch2b"
  top: "bn4b_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_27"
  type: "ReLU"
  bottom: "bn4b_branch2b"
  top: "activation_27"
}
layer {
  name: "res4b_branch2c"
  type: "Convolution"
  bottom: "activation_27"
  top: "res4b_branch2c"
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.644979
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.1336339
  }
}
layer {
  name: "bn4b_branch2c"
  type: "BatchNorm"
  bottom: "res4b_branch2c"
  top: "bn4b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4b_branch2c_scale"
  type: "Scale"
  bottom: "bn4b_branch2c"
  top: "bn4b_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_9"
  type: "Eltwise"
  bottom: "bn4b_branch2c"
  bottom: "activation_25"
  top: "add_9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_28"
  type: "ReLU"
  bottom: "add_9"
  top: "activation_28"
}
layer {
  name: "res4c_branch2a"
  type: "Convolution"
  bottom: "activation_28"
  top: "res4c_branch2a"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.1896917
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.2822067
  }
}
layer {
  name: "bn4c_branch2a"
  type: "BatchNorm"
  bottom: "res4c_branch2a"
  top: "bn4c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4c_branch2a_scale"
  type: "Scale"
  bottom: "bn4c_branch2a"
  top: "bn4c_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_29"
  type: "ReLU"
  bottom: "bn4c_branch2a"
  top: "activation_29"
}
layer {
  name: "res4c_branch2b"
  type: "Convolution"
  bottom: "activation_29"
  top: "res4c_branch2b"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.5614756
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.9259106
  }
}
layer {
  name: "bn4c_branch2b"
  type: "BatchNorm"
  bottom: "res4c_branch2b"
  top: "bn4c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4c_branch2b_scale"
  type: "Scale"
  bottom: "bn4c_branch2b"
  top: "bn4c_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_30"
  type: "ReLU"
  bottom: "bn4c_branch2b"
  top: "activation_30"
}
layer {
  name: "res4c_branch2c"
  type: "Convolution"
  bottom: "activation_30"
  top: "res4c_branch2c"
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.9135047
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.7121775
  }
}
layer {
  name: "bn4c_branch2c"
  type: "BatchNorm"
  bottom: "res4c_branch2c"
  top: "bn4c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4c_branch2c_scale"
  type: "Scale"
  bottom: "bn4c_branch2c"
  top: "bn4c_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_10"
  type: "Eltwise"
  bottom: "bn4c_branch2c"
  bottom: "activation_28"
  top: "add_10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_31"
  type: "ReLU"
  bottom: "add_10"
  top: "activation_31"
}
layer {
  name: "res4d_branch2a"
  type: "Convolution"
  bottom: "activation_31"
  top: "res4d_branch2a"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.2073647
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.6793746
  }
}
layer {
  name: "bn4d_branch2a"
  type: "BatchNorm"
  bottom: "res4d_branch2a"
  top: "bn4d_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4d_branch2a_scale"
  type: "Scale"
  bottom: "bn4d_branch2a"
  top: "bn4d_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_32"
  type: "ReLU"
  bottom: "bn4d_branch2a"
  top: "activation_32"
}
layer {
  name: "res4d_branch2b"
  type: "Convolution"
  bottom: "activation_32"
  top: "res4d_branch2b"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.464662
  }
  blobs_dtype {
    type: DT_INT8
    position: -9
    scale: 1.2313687
  }
}
layer {
  name: "bn4d_branch2b"
  type: "BatchNorm"
  bottom: "res4d_branch2b"
  top: "bn4d_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4d_branch2b_scale"
  type: "Scale"
  bottom: "bn4d_branch2b"
  top: "bn4d_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_33"
  type: "ReLU"
  bottom: "bn4d_branch2b"
  top: "activation_33"
}
layer {
  name: "res4d_branch2c"
  type: "Convolution"
  bottom: "activation_33"
  top: "res4d_branch2c"
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.5359932
  }
  blobs_dtype {
    type: DT_INT8
    position: -9
    scale: 1.0358756
  }
}
layer {
  name: "bn4d_branch2c"
  type: "BatchNorm"
  bottom: "res4d_branch2c"
  top: "bn4d_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4d_branch2c_scale"
  type: "Scale"
  bottom: "bn4d_branch2c"
  top: "bn4d_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_11"
  type: "Eltwise"
  bottom: "bn4d_branch2c"
  bottom: "activation_31"
  top: "add_11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_34"
  type: "ReLU"
  bottom: "add_11"
  top: "activation_34"
}
layer {
  name: "res4e_branch2a"
  type: "Convolution"
  bottom: "activation_34"
  top: "res4e_branch2a"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.1792018
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.6206594
  }
}
layer {
  name: "bn4e_branch2a"
  type: "BatchNorm"
  bottom: "res4e_branch2a"
  top: "bn4e_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4e_branch2a_scale"
  type: "Scale"
  bottom: "bn4e_branch2a"
  top: "bn4e_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_35"
  type: "ReLU"
  bottom: "bn4e_branch2a"
  top: "activation_35"
}
layer {
  name: "res4e_branch2b"
  type: "Convolution"
  bottom: "activation_35"
  top: "res4e_branch2b"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.7810072
  }
  blobs_dtype {
    type: DT_INT8
    position: -9
    scale: 1.1118652
  }
}
layer {
  name: "bn4e_branch2b"
  type: "BatchNorm"
  bottom: "res4e_branch2b"
  top: "bn4e_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4e_branch2b_scale"
  type: "Scale"
  bottom: "bn4e_branch2b"
  top: "bn4e_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_36"
  type: "ReLU"
  bottom: "bn4e_branch2b"
  top: "activation_36"
}
layer {
  name: "res4e_branch2c"
  type: "Convolution"
  bottom: "activation_36"
  top: "res4e_branch2c"
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.9798
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.8723063
  }
}
layer {
  name: "bn4e_branch2c"
  type: "BatchNorm"
  bottom: "res4e_branch2c"
  top: "bn4e_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4e_branch2c_scale"
  type: "Scale"
  bottom: "bn4e_branch2c"
  top: "bn4e_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_12"
  type: "Eltwise"
  bottom: "bn4e_branch2c"
  bottom: "activation_34"
  top: "add_12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_37"
  type: "ReLU"
  bottom: "add_12"
  top: "activation_37"
}
layer {
  name: "res4f_branch2a"
  type: "Convolution"
  bottom: "activation_37"
  top: "res4f_branch2a"
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.9207817
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.5046995
  }
}
layer {
  name: "bn4f_branch2a"
  type: "BatchNorm"
  bottom: "res4f_branch2a"
  top: "bn4f_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4f_branch2a_scale"
  type: "Scale"
  bottom: "bn4f_branch2a"
  top: "bn4f_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_38"
  type: "ReLU"
  bottom: "bn4f_branch2a"
  top: "activation_38"
}
layer {
  name: "res4f_branch2b"
  type: "Convolution"
  bottom: "activation_38"
  top: "res4f_branch2b"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.0093868
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.6948521
  }
}
layer {
  name: "bn4f_branch2b"
  type: "BatchNorm"
  bottom: "res4f_branch2b"
  top: "bn4f_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4f_branch2b_scale"
  type: "Scale"
  bottom: "bn4f_branch2b"
  top: "bn4f_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_39"
  type: "ReLU"
  bottom: "bn4f_branch2b"
  top: "activation_39"
}
layer {
  name: "res4f_branch2c"
  type: "Convolution"
  bottom: "activation_39"
  top: "res4f_branch2c"
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.4329385
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.6900052
  }
}
layer {
  name: "bn4f_branch2c"
  type: "BatchNorm"
  bottom: "res4f_branch2c"
  top: "bn4f_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn4f_branch2c_scale"
  type: "Scale"
  bottom: "bn4f_branch2c"
  top: "bn4f_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_13"
  type: "Eltwise"
  bottom: "bn4f_branch2c"
  bottom: "activation_37"
  top: "add_13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_40"
  type: "ReLU"
  bottom: "add_13"
  top: "activation_40"
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "activation_40"
  top: "res5a_branch2a"
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 2
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.8964617
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.498861
  }
}
layer {
  name: "bn5a_branch2a"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "bn5a_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5a_branch2a_scale"
  type: "Scale"
  bottom: "bn5a_branch2a"
  top: "bn5a_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_41"
  type: "ReLU"
  bottom: "bn5a_branch2a"
  top: "activation_41"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "activation_41"
  top: "res5a_branch2b"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.1817882
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.8272326
  }
}
layer {
  name: "bn5a_branch2b"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "bn5a_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5a_branch2b_scale"
  type: "Scale"
  bottom: "bn5a_branch2b"
  top: "bn5a_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_42"
  type: "ReLU"
  bottom: "bn5a_branch2b"
  top: "activation_42"
}
layer {
  name: "res5a_branch2c"
  type: "Convolution"
  bottom: "activation_42"
  top: "res5a_branch2c"
  convolution_param {
    num_output: 2048
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.4754703
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.1489869
  }
}
layer {
  name: "res5a_branch1"
  type: "Convolution"
  bottom: "activation_40"
  top: "res5a_branch1"
  convolution_param {
    num_output: 2048
    pad: 0
    kernel_size: 1
    stride: 2
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -1
    scale: 1.8964617
  }
  blobs_dtype {
    type: DT_INT8
    position: -7
    scale: 1.5948071
  }
}
layer {
  name: "bn5a_branch2c"
  type: "BatchNorm"
  bottom: "res5a_branch2c"
  top: "bn5a_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5a_branch2c_scale"
  type: "Scale"
  bottom: "bn5a_branch2c"
  top: "bn5a_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "bn5a_branch1"
  type: "BatchNorm"
  bottom: "res5a_branch1"
  top: "bn5a_branch1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5a_branch1_scale"
  type: "Scale"
  bottom: "bn5a_branch1"
  top: "bn5a_branch1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_14"
  type: "Eltwise"
  bottom: "bn5a_branch2c"
  bottom: "bn5a_branch1"
  top: "add_14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_43"
  type: "ReLU"
  bottom: "add_14"
  top: "activation_43"
}
layer {
  name: "res5b_branch2a"
  type: "Convolution"
  bottom: "activation_43"
  top: "res5b_branch2a"
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 1.3947768
  }
  blobs_dtype {
    type: DT_INT8
    position: -7
    scale: 1.7205372
  }
}
layer {
  name: "bn5b_branch2a"
  type: "BatchNorm"
  bottom: "res5b_branch2a"
  top: "bn5b_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5b_branch2a_scale"
  type: "Scale"
  bottom: "bn5b_branch2a"
  top: "bn5b_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_44"
  type: "ReLU"
  bottom: "bn5b_branch2a"
  top: "activation_44"
}
layer {
  name: "res5b_branch2b"
  type: "Convolution"
  bottom: "activation_44"
  top: "res5b_branch2b"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.2120801
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.7476504
  }
}
layer {
  name: "bn5b_branch2b"
  type: "BatchNorm"
  bottom: "res5b_branch2b"
  top: "bn5b_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5b_branch2b_scale"
  type: "Scale"
  bottom: "bn5b_branch2b"
  top: "bn5b_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_45"
  type: "ReLU"
  bottom: "bn5b_branch2b"
  top: "activation_45"
}
layer {
  name: "res5b_branch2c"
  type: "Convolution"
  bottom: "activation_45"
  top: "res5b_branch2c"
  convolution_param {
    num_output: 2048
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.5363277
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.7927005
  }
}
layer {
  name: "bn5b_branch2c"
  type: "BatchNorm"
  bottom: "res5b_branch2c"
  top: "bn5b_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5b_branch2c_scale"
  type: "Scale"
  bottom: "bn5b_branch2c"
  top: "bn5b_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_15"
  type: "Eltwise"
  bottom: "bn5b_branch2c"
  bottom: "activation_43"
  top: "add_15"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_46"
  type: "ReLU"
  bottom: "add_15"
  top: "activation_46"
}
layer {
  name: "res5c_branch2a"
  type: "Convolution"
  bottom: "activation_46"
  top: "res5c_branch2a"
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 1.2119981
  }
  blobs_dtype {
    type: DT_INT8
    position: -7
    scale: 1.9285927
  }
}
layer {
  name: "bn5c_branch2a"
  type: "BatchNorm"
  bottom: "res5c_branch2a"
  top: "bn5c_branch2a"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5c_branch2a_scale"
  type: "Scale"
  bottom: "bn5c_branch2a"
  top: "bn5c_branch2a"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_47"
  type: "ReLU"
  bottom: "bn5c_branch2a"
  top: "activation_47"
}
layer {
  name: "res5c_branch2b"
  type: "Convolution"
  bottom: "activation_47"
  top: "res5c_branch2b"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -3
    scale: 1.1221565
  }
  blobs_dtype {
    type: DT_INT8
    position: -9
    scale: 1.7254449
  }
}
layer {
  name: "bn5c_branch2b"
  type: "BatchNorm"
  bottom: "res5c_branch2b"
  top: "bn5c_branch2b"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5c_branch2b_scale"
  type: "Scale"
  bottom: "bn5c_branch2b"
  top: "bn5c_branch2b"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "activation_48"
  type: "ReLU"
  bottom: "bn5c_branch2b"
  top: "activation_48"
}
layer {
  name: "res5c_branch2c"
  type: "Convolution"
  bottom: "activation_48"
  top: "res5c_branch2c"
  convolution_param {
    num_output: 2048
    pad: 0
    kernel_size: 1
    stride: 1
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: -2
    scale: 1.1437495
  }
  blobs_dtype {
    type: DT_INT8
    position: -8
    scale: 1.6538273
  }
}
layer {
  name: "bn5c_branch2c"
  type: "BatchNorm"
  bottom: "res5c_branch2c"
  top: "bn5c_branch2c"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.001
  }
}
layer {
  name: "bn5c_branch2c_scale"
  type: "Scale"
  bottom: "bn5c_branch2c"
  top: "bn5c_branch2c"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "add_16"
  type: "Eltwise"
  bottom: "bn5c_branch2c"
  bottom: "activation_46"
  top: "add_16"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "activation_49"
  type: "ReLU"
  bottom: "add_16"
  top: "activation_49"
}
layer {
  name: "avg_pool"
  type: "Pooling"
  bottom: "activation_49"
  top: "avg_pool"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "avg_pool"
  top: "fc1000"
  inner_product_param {
    num_output: 1000
  }
  bottom_mlu_dtype {
    type: DT_INT8
    position: 0
    scale: 1.9987001
  }
  blobs_dtype {
    type: DT_INT8
    position: -7
    scale: 1.3477898
  }
}
layer {
  name: "fc1000_relu"
  type: "Softmax"
  bottom: "fc1000"
  top: "fc1000"
}
